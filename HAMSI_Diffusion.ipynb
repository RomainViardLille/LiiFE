{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cce4e26-c18a-4099-805e-e3af185df954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "import os\n",
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "#from nibabel import load as load_nii\n",
    "import nibabel as nib\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79560a23-50ee-456c-bc41-bd50b8f9297a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NiftiiDataset(Dataset):\n",
    "    def __init__(self, source_paths, target_paths):\n",
    "        self.source_slices = []\n",
    "        self.target_slices = []\n",
    "\n",
    "        for source_path, target_path in zip(source_paths, target_paths):\n",
    "            source_nii = nib.load(source_path)\n",
    "            target_nii = nib.load(target_path)\n",
    "\n",
    "            source_img = torch.tensor(source_nii.get_fdata(dtype=np.float32))\n",
    "            target_img = torch.tensor(target_nii.get_fdata(dtype=np.float32))\n",
    "\n",
    "            source_slice = source_img[:, :, source_img.shape[2] // 2].unsqueeze(0)\n",
    "            target_slice = target_img[:, :, target_img.shape[2] // 2].unsqueeze(0)\n",
    "\n",
    "            self.source_slices.append(source_slice)\n",
    "            self.target_slices.append(target_slice)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_slices[idx], self.target_slices[idx]\n",
    "\n",
    "def load_data():\n",
    "    source_image_paths = sorted(glob.glob(\"/home/youssef/harmo_4/ALL_training_data/Pat*_CHU_zscore_minmax_unbias.nii.gz\"))\n",
    "    target_image_paths = sorted(glob.glob(\"/home/youssef/harmo_4/ALL_training_data/Pat*_COL_zscore_minmax_unbias.nii.gz\"))\n",
    "\n",
    "    dataset = NiftiiDataset(source_image_paths, target_image_paths)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2bfd471-0a97-42fe-af11-ef684c149a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    model = Unet(\n",
    "        #dim = 64, #for better result put it to 128\n",
    "        #dim_mults = (1, 2, 4, 8), #for better result Add an additional layer (1, 2, 4, 8, 16)\n",
    "        dim = 128,\n",
    "        dim_mults = (1, 2, 4, 8, 16),\n",
    "        channels=1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def define_diffusion(model):\n",
    "    diffusion = GaussianDiffusion(\n",
    "        model=model,\n",
    "        image_size=256,\n",
    "        timesteps=1000\n",
    "    )\n",
    "    return diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "375005a3-82db-4aa4-9962-45110942b30d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span>target_image_path = <span style=\"color: #808000; text-decoration-color: #808000\">\"/home/youssef/harmo_4/test_data/Pat42_COL_zscore_minmax_unbias.nii.</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span>model_path = <span style=\"color: #808000; text-decoration-color: #808000\">\"/home/youssef/harmo_4/trained_model/savedmodel_02.pt\"</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>33 generate_harmonized_image(model_path, source_image_path, target_image_path)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 #generate_harmonized_image('savedmodel.pt', source_image_path, target_image_path)</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_harmonized_image</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_harmonized_image</span>(model_path, source_image_path, target_image_path):            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Load the trained model</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model = define_model()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model.load_state_dict(torch.load(model_path))                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model.eval()                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">define_model</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#dim_mults = (1, 2, 4, 8), #for better result Add an additional layer (1, 2, 4, </span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>dim = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">128</span>,                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>dim_mults = (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>),                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>channels=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/romain/.local/lib/python3.7/site-packages/denoising_diffusion_pytorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">denoising_diffusion_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pytorch.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">335</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 332 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_heads = cast_tuple(attn_heads, num_stages)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 333 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_dim_head = cast_tuple(attn_dim_head, num_stages)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 334 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 335 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(full_attn) == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(dim_mults)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>FullAttention = partial(Attention, flash = flash_attn)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 338 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m33\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0mtarget_image_path = \u001b[33m\"\u001b[0m\u001b[33m/home/youssef/harmo_4/test_data/Pat42_COL_zscore_minmax_unbias.nii.\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0mmodel_path = \u001b[33m\"\u001b[0m\u001b[33m/home/youssef/harmo_4/trained_model/savedmodel_02.pt\u001b[0m\u001b[33m\"\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m33 generate_harmonized_image(model_path, source_image_path, target_image_path)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m#generate_harmonized_image('savedmodel.pt', source_image_path, target_image_path)\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgenerate_harmonized_image\u001b[0m:\u001b[94m3\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgenerate_harmonized_image\u001b[0m(model_path, source_image_path, target_image_path):            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Load the trained model\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 3 \u001b[2m│   \u001b[0mmodel = define_model()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0mmodel.load_state_dict(torch.load(model_path))                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0mmodel.eval()                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mdefine_model\u001b[0m:\u001b[94m7\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#dim_mults = (1, 2, 4, 8), #for better result Add an additional layer (1, 2, 4, \u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   │   \u001b[0mdim = \u001b[94m128\u001b[0m,                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   │   \u001b[0mdim_mults = (\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m, \u001b[94m4\u001b[0m, \u001b[94m8\u001b[0m, \u001b[94m16\u001b[0m),                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 \u001b[2m│   │   \u001b[0mchannels=\u001b[94m1\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m model                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/romain/.local/lib/python3.7/site-packages/denoising_diffusion_pytorch/\u001b[0m\u001b[1;33mdenoising_diffusion_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpytorch.py\u001b[0m:\u001b[94m335\u001b[0m in \u001b[92m__init__\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 332 \u001b[0m\u001b[2m│   │   \u001b[0mattn_heads = cast_tuple(attn_heads, num_stages)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 333 \u001b[0m\u001b[2m│   │   \u001b[0mattn_dim_head = cast_tuple(attn_dim_head, num_stages)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 334 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 335 \u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(full_attn) == \u001b[96mlen\u001b[0m(dim_mults)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 336 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 337 \u001b[0m\u001b[2m│   │   \u001b[0mFullAttention = partial(Attention, flash = flash_attn)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 338 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_harmonized_image(model_path, source_slice, target_slice):\n",
    "    # Load the trained model\n",
    "    model = define_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Generate a random timestep\n",
    "    timesteps = 1000\n",
    "    t = torch.randint(0, timesteps, (source_slice.size(0),), device=source_slice.device)\n",
    "\n",
    "    # Generate the harmonized image\n",
    "    with torch.no_grad():\n",
    "        harmonized_slice = model(source_slice, t)\n",
    "\n",
    "    # Save the harmonized image\n",
    "    save_image(harmonized_slice, 'harmonized_slice8.png')\n",
    "    \n",
    "def generate_harmonized_image(model_path, source_image_path, target_image_path):\n",
    "    # Load the trained model\n",
    "    model = define_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Load the source and target images\n",
    "    source_nii = nib.load(source_image_path)\n",
    "    target_nii = nib.load(target_image_path)\n",
    "\n",
    "    source_img = torch.tensor(source_nii.get_fdata(dtype=np.float32))\n",
    "    target_img = torch.tensor(target_nii.get_fdata(dtype=np.float32))\n",
    "\n",
    "    source_slice = source_img[:, :, source_img.shape[2] // 2].unsqueeze(0).unsqueeze(0)\n",
    "    target_slice = target_img[:, :, target_img.shape[2] // 2].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Generate a random timestep\n",
    "    timesteps = 1000\n",
    "    t = torch.randint(0, timesteps, (source_slice.size(0),), device=source_slice.device)\n",
    "\n",
    "    # Generate the harmonized image\n",
    "    with torch.no_grad():\n",
    "        harmonized_slice = model(source_slice, t)\n",
    "\n",
    "    # Save the harmonized image\n",
    "    save_image(harmonized_slice, '/home/youssef/harmo_4/harmonized_result/harmonized_slice42_9.png')\n",
    "\n",
    "\n",
    "source_image_path = \"/home/youssef/harmo_4/test_data/Pat42_CHU_zscore_minmax_unbias.nii.gz\"\n",
    "target_image_path = \"/home/youssef/harmo_4/test_data/Pat42_COL_zscore_minmax_unbias.nii.gz\"\n",
    "model_path = \"/home/youssef/harmo_4/trained_model/savedmodel_02.pt\"\n",
    "\n",
    "generate_harmonized_image(model_path, source_image_path, target_image_path)\n",
    "#generate_harmonized_image('savedmodel.pt', source_image_path, target_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c96f55-d092-4ddc-bebb-023528211597",
   "metadata": {},
   "source": [
    "training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f988e4f-6e77-44c6-9327-6c6f1338ed09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, diffusion):\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(10): \n",
    "        for source_slice, target_slice in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate a random timestep for each batch\n",
    "            timesteps = 1000\n",
    "            t = torch.randint(0, timesteps, (source_slice.size(0),), device=source_slice.device)\n",
    "\n",
    "            # The model's forward call now includes the timestep\n",
    "            reconstructed_slice = model(source_slice, t)\n",
    "\n",
    "            # Calculate loss, backpropagate, and update model weights as before\n",
    "            loss = criterion(reconstructed_slice, target_slice)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'savedmodel.pt')\n",
    "\n",
    "\n",
    "def train_model(dataloader, model, diffusion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    scaler = GradScaler()  # Initialize GradScaler\n",
    "\n",
    "    for epoch in range(100): \n",
    "        for source_slice, target_slice in dataloader:\n",
    "            source_slice, target_slice = source_slice.to(device), target_slice.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            timesteps = 1000\n",
    "            t = torch.randint(0, timesteps, (source_slice.size(0),), device=source_slice.device)\n",
    "\n",
    "            # Use autocast to run the forward pass in mixed precision\n",
    "            with autocast():\n",
    "                reconstructed_slice = model(source_slice, t)\n",
    "                loss = criterion(reconstructed_slice, target_slice)\n",
    "\n",
    "            # Use GradScaler to scale the loss and call backward\n",
    "            scaler.scale(loss).backward()\n",
    "            # Use GradScaler to step the optimizer\n",
    "            scaler.step(optimizer)\n",
    "            # Update the scale for next iteration\n",
    "            scaler.update()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343afbc-0bbd-4c24-a2ba-ab41b0cd5420",
   "metadata": {},
   "source": [
    "test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bc77f1d-f73c-4eb0-90dc-59ca44f54963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_images(source_path, target_path):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    source = transform(Image.open(source_path))\n",
    "    target = transform(Image.open(target_path))\n",
    "    return source.unsqueeze(0), target.unsqueeze(0)\n",
    "\n",
    "def harmonize_images(model, source, target):\n",
    "    with torch.no_grad():\n",
    "        harmonized = model(source, target)\n",
    "    return harmonized.squeeze(0)\n",
    "\n",
    "def save_image(image, save_path):\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    image.save(save_path)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', required=True)\n",
    "    parser.add_argument('--source', required=True)\n",
    "    parser.add_argument('--target', required=True)\n",
    "    parser.add_argument('--save', required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = load_model(args.model)\n",
    "    source, target = load_images(args.source, args.target)\n",
    "    harmonized = harmonize_images(model, source, target)\n",
    "    save_image(harmonized, args.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb77d1-00ca-49a3-9967-96c46f687598",
   "metadata": {
    "tags": []
   },
   "source": [
    "original.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2f5c31d-3e4c-4f58-8711-452147cfc537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7744356393814087\n",
      "Epoch 2, Loss: 0.22553113102912903\n",
      "Epoch 3, Loss: 0.23664915561676025\n",
      "Epoch 4, Loss: 0.10385526716709137\n",
      "Epoch 5, Loss: 0.18706238269805908\n",
      "Epoch 6, Loss: 0.19578926265239716\n",
      "Epoch 7, Loss: 0.12775063514709473\n",
      "Epoch 8, Loss: 0.09209910035133362\n",
      "Epoch 9, Loss: 0.06490813195705414\n",
      "Epoch 10, Loss: 0.043991755694150925\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class NiftiiDataset(Dataset):\n",
    "    def __init__(self, source_path, target_path):\n",
    "        source_nii = nib.load(source_path)\n",
    "        target_nii = nib.load(target_path)\n",
    "        self.source_img = torch.tensor(source_nii.get_fdata(dtype=np.float32))\n",
    "        self.target_img = torch.tensor(target_nii.get_fdata(dtype=np.float32))\n",
    "        self.source_slice = self.source_img[:, :, self.source_img.shape[2] // 2].unsqueeze(0)\n",
    "        self.target_slice = self.target_img[:, :, self.target_img.shape[2] // 2].unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_slice, self.target_slice\n",
    "\n",
    "def load_data(source_image_path, target_image_path):\n",
    "    dataset = NiftiiDataset(source_image_path, target_image_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=1)\n",
    "    return dataloader\n",
    "\n",
    "def define_model():\n",
    "    model = Unet(\n",
    "        dim = 64,\n",
    "        dim_mults = (1, 2, 4, 8),\n",
    "        channels=1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def define_diffusion(model):\n",
    "    diffusion = GaussianDiffusion(\n",
    "        model=model,\n",
    "        image_size=256,\n",
    "        timesteps=1000\n",
    "    )\n",
    "    return diffusion\n",
    "\n",
    "def train_model(dataloader, model, diffusion):\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(10): \n",
    "        for source_slice, target_slice in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate a random timestep for each batch\n",
    "            timesteps = 1000\n",
    "            t = torch.randint(0, timesteps, (source_slice.size(0),), device=source_slice.device)\n",
    "\n",
    "            # The model's forward call now includes the timestep\n",
    "            reconstructed_slice = model(source_slice, t)\n",
    "\n",
    "            # Calculate loss, backpropagate, and update model weights as before\n",
    "            loss = criterion(reconstructed_slice, target_slice)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'savedmodel.pt')\n",
    "\n",
    "\n",
    "def generate_harmonized_image(model_path, source_slice, target_slice):\n",
    "    # Load the trained model\n",
    "    model = define_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Generate a random timestep\n",
    "    timesteps = 1000\n",
    "    t = torch.randint(0, timesteps, (source_slice.size(0),), device=source_slice.device)\n",
    "\n",
    "    # Generate the harmonized image\n",
    "    with torch.no_grad():\n",
    "        harmonized_slice = model(source_slice, t)\n",
    "\n",
    "    # Save the harmonized image\n",
    "    save_image(harmonized_slice, 'harmonized_slice8.png')\n",
    "\n",
    "\n",
    "def main():\n",
    "    source_image_path = \"/home/youssef/harmonization_project/data/train_2/Pat8_CHU_zscore_minmax_unbias.nii.gz\"\n",
    "    target_image_path = \"/home/youssef/harmonization_project/data/train_2/Pat8_COL_zscore_minmax_unbias.nii.gz\"\n",
    "\n",
    "    dataloader = load_data(source_image_path, target_image_path)\n",
    "    model = define_model()\n",
    "    diffusion = define_diffusion(model)\n",
    "    train_model(dataloader, model, diffusion)\n",
    "\n",
    "    for source_slice, target_slice in dataloader:\n",
    "        generate_harmonized_image('savedmodel.pt', source_slice, target_slice)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b4989-0023-4994-ba37-8e1695a431e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = \"/home/youssef/harmo_4/test_data/Pat42_CHU_zscore_minmax_unbias.nii.gz\"\n",
    "target_image_path = \"/home/youssef/harmo_4/test_data/Pat42_COL_zscore_minmax_unbias.nii.gz\"\n",
    "model_path = \"/home/youssef/harmo_4/trained_model/savedmodel_02.pt\"\n",
    "\n",
    "generate_harmonized_image(model_path, source_image_path, target_image_path)\n",
    "#generate_harmonized_image('savedmodel.pt', source_image_path, target_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5e214-62d3-4853-8645-89cd2aca95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
